{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo_train = 'Data/trainReg.txt'\n",
    "ruta_archivo_test = 'Data/testReg.txt'\n",
    "\n",
    "train_tot = pd.read_csv(ruta_archivo_train, delimiter=',')\n",
    "test_tot = pd.read_csv(ruta_archivo_test, delimiter=',')\n",
    "\n",
    "\n",
    "x_train_ori = train_tot[train_tot.columns[~train_tot.columns.isin([\"V1\", \"V4\"])]]\n",
    "y_train_ori =train_tot[\"V1\"]\n",
    "\n",
    "x_test_ori =test_tot[test_tot.columns[~test_tot.columns.isin([\"Id\", \"V4\"])]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train_ori, y_train_ori, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((54445, 89), (23334, 89), (54445,), (23334,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train_scale = scaler.transform(x_train)\n",
    "x_test_scale = scaler.transform(x_test)\n",
    "x_test_scale_ori = scaler.transform(x_test_ori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables de Timbre y Covarianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar variables de timbre y covarianza\n",
    "x_train_scale_timbre = x_train_scale[:, 0:11]\n",
    "x_train_scale_covarianza = x_train_scale[:, 11:]\n",
    "\n",
    "x_test_scale_timbre = x_test_scale[:, 0:11]\n",
    "x_test_scale_covarianza = x_test_scale[:, 11:]\n",
    "\n",
    "x_test_scale_timbre_ori = x_test_scale_ori[:, 0:11]\n",
    "x_test_scale_covarianza_ori = x_test_scale_ori[:, 11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54445, 78)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scale_covarianza.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Timbre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA al timbre\n",
    "n_components = 11\n",
    "\n",
    "pca_timbre = PCA(n_components=n_components)\n",
    "pca_timbre.fit(x_train_scale_timbre)\n",
    "\n",
    "x_train_pca_timbre = pca_timbre.transform(x_train_scale_timbre)\n",
    "x_test_pca_timbre = pca_timbre.transform(x_test_scale_timbre)\n",
    "x_test_pca_timbre_ori = pca_timbre.transform(x_test_scale_timbre_ori)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Comentarrrr\n",
    "# x_train_pca_timbre = (x_train_scale_timbre)\n",
    "# x_test_pca_timbre = (x_test_scale_timbre)\n",
    "# x_test_pca_timbre_ori = (x_test_scale_timbre_ori)\n",
    "# ------------------------------------------------\n",
    "\n",
    "# DataFrame\n",
    "columns = [f\"PC-T-{i}\" for i in range(1, n_components + 1)]\n",
    "x_train_pca_timbre_df = pd.DataFrame(x_train_pca_timbre, columns=columns)\n",
    "x_test_pca_timbre_df = pd.DataFrame(x_test_pca_timbre, columns=columns)\n",
    "x_test_pca_timbre_ori_df = pd.DataFrame(x_test_pca_timbre_ori, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - Covarianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA al timbre\n",
    "n_components = 78\n",
    "\n",
    "pca_covarianza = PCA(n_components=n_components)\n",
    "pca_covarianza.fit(x_train_scale_covarianza)\n",
    "\n",
    "x_train_pca_covarianza = pca_covarianza.transform(x_train_scale_covarianza)\n",
    "x_test_pca_covarianza = pca_covarianza.transform(x_test_scale_covarianza)\n",
    "x_test_pca_covarianza_ori = pca_covarianza.transform(x_test_scale_covarianza_ori)\n",
    "\n",
    "# DataFrame\n",
    "columns = [f\"PC-C-{i}\" for i in range(1, n_components + 1)]\n",
    "x_train_pca_covarianza_df = pd.DataFrame(x_train_pca_covarianza, columns=columns)\n",
    "x_test_pca_covarianza_df = pd.DataFrame(x_test_pca_covarianza, columns=columns)\n",
    "x_test_pca_covarianza_ori_df = pd.DataFrame(x_test_pca_covarianza_ori, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pca_df = pd.concat([x_train_pca_timbre_df, x_train_pca_covarianza_df], axis=1)\n",
    "x_test_pca_df = pd.concat([x_test_pca_timbre_df, x_test_pca_covarianza_df], axis=1)\n",
    "x_test_pca_ori_df = pd.concat([x_test_pca_timbre_ori_df, x_test_pca_covarianza_ori_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aplicar PCA a los datos de entrenamiento y prueba con 23 componentes\n",
    "# n_components = 89\n",
    "\n",
    "# pca = PCA(n_components=n_components)\n",
    "# pca.fit(x_train_scale)\n",
    "\n",
    "# x_train_pca = pca.transform(x_train_scale)\n",
    "# x_test_pca = pca.transform(x_test_scale)\n",
    "# x_test_pca_ori = pca.transform(x_test_scale_ori)\n",
    "# # Convert NumPy arrays to Pandas DataFrames\n",
    "# columns = [f\"PC{i}\" for i in range(1, n_components + 1)]\n",
    "# x_train_pca_df = pd.DataFrame(x_train_pca, columns=columns)\n",
    "# x_test_pca_df = pd.DataFrame(x_test_pca, columns=columns)\n",
    "# x_test_pca_ori_df = pd.DataFrame(x_test_pca_ori, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(alpha=0.01, l1_ratio=0.2, penalty=&#x27;elasticnet&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" checked><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(alpha=0.01, l1_ratio=0.2, penalty=&#x27;elasticnet&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(alpha=0.01, l1_ratio=0.2, penalty='elasticnet', random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Gradient Boosting Regressor model\n",
    "model = SGDRegressor(\n",
    "    random_state=42,\n",
    "    penalty='elasticnet',\n",
    "    alpha=0.01,\n",
    "    l1_ratio=0.2,\n",
    "    \n",
    ")\n",
    "# Train the model\n",
    "model.fit(x_train_pca_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse\n",
      "89.73938973270525\n",
      "mse\n",
      "90.85507505081696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = model.predict(x_train_pca_df)\n",
    "y_pred_test = model.predict(x_test_pca_df)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"mse\")\n",
    "print(mse_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"mse\")\n",
    "print(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "y_pred_test_ori = model.predict(x_test_pca_ori_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id            y\n",
      "0   1  2003.900833\n",
      "1   2  1997.359920\n",
      "2   3  2000.168710\n",
      "3   4  2002.754024\n",
      "4   5  2004.414182\n"
     ]
    }
   ],
   "source": [
    "# Create the results DataFrame\n",
    "results = pd.DataFrame({\"Id\": test_tot[\"Id\"], \"y\": y_pred_test_ori.flatten()})\n",
    "\n",
    "# Print the first few rows of the results DataFrame\n",
    "print(results.head())\n",
    "\n",
    "# Guardar el DataFrame en un archivo CSV\n",
    "# ruta_resultados = r'C:\\Users\\Usuario\\Downloads\\Statistical\\Statistical\\proy\\Regre\\boost11100.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_resultados = f'resultados_rf_100.csv'\n",
    "\n",
    "results.to_csv(ruta_resultados, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_new'] = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_1'] = df['y_new'].round()\n",
    "df['y'] = df['y_new_new'].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>y</th>\n",
       "      <th>y_new</th>\n",
       "      <th>y_new_new</th>\n",
       "      <th>y_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2006.405057</td>\n",
       "      <td>2006.005057</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1998.716301</td>\n",
       "      <td>1998.316301</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>2005.587338</td>\n",
       "      <td>2005.987338</td>\n",
       "      <td>2006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.052711</td>\n",
       "      <td>2001.652711</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>2004.248900</td>\n",
       "      <td>2004.648900</td>\n",
       "      <td>2004.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18652</th>\n",
       "      <td>18653</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1997.836605</td>\n",
       "      <td>1997.436605</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18653</th>\n",
       "      <td>18654</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.856300</td>\n",
       "      <td>2002.456300</td>\n",
       "      <td>2003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18654</th>\n",
       "      <td>18655</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1999.269548</td>\n",
       "      <td>1999.669548</td>\n",
       "      <td>1999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18655</th>\n",
       "      <td>18656</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2000.907672</td>\n",
       "      <td>2000.507672</td>\n",
       "      <td>2001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18656</th>\n",
       "      <td>18657</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1998.117202</td>\n",
       "      <td>1997.717202</td>\n",
       "      <td>1998.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18657 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Id       y        y_new    y_new_new     y_1\n",
       "0          1  2006.0  2006.405057  2006.005057  2006.0\n",
       "1          2  1998.0  1998.716301  1998.316301  1999.0\n",
       "2          3  2006.0  2005.587338  2005.987338  2006.0\n",
       "3          4  2002.0  2002.052711  2001.652711  2002.0\n",
       "4          5  2005.0  2004.248900  2004.648900  2004.0\n",
       "...      ...     ...          ...          ...     ...\n",
       "18652  18653  1997.0  1997.836605  1997.436605  1998.0\n",
       "18653  18654  2002.0  2002.856300  2002.456300  2003.0\n",
       "18654  18655  2000.0  1999.269548  1999.669548  1999.0\n",
       "18655  18656  2001.0  2000.907672  2000.507672  2001.0\n",
       "18656  18657  1998.0  1998.117202  1997.717202  1998.0\n",
       "\n",
       "[18657 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_resultados = f'y_test.csv'\n",
    "\n",
    "df[['Id', 'y']].to_csv(ruta_resultados, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_changes = np.random.choice([-0.4, 0.4], size=len(df['y_new']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_new_new'] = df['y_new'] + random_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\SGM\\OneDrive\\Documentos\\Santiago González Montealegre\\Materias Uniandes\\2023-20\\Statistical Learning\\Proyecto\\Regresion\\b.ipynb Cell 10\u001b[0m line \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     estimator\u001b[39m=\u001b[39mrf_model,\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     param_grid\u001b[39m=\u001b[39mparam_grid, \n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Paso 6: Ajustar el modelo utilizando GridSearchCV\u001b[39;00m\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/SGM/OneDrive/Documentos/Santiago%20Gonz%C3%A1lez%20Montealegre/Materias%20Uniandes/2023-20/Statistical%20Learning/Proyecto/Regresion/b.ipynb#X31sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(x_train_pca_df, y_train)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n",
      "\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n",
      "\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n",
      "\u001b[0;32m    871\u001b[0m     )\n",
      "\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n",
      "\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n",
      "\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n",
      "\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n",
      "\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n",
      "\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n",
      "\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n",
      "\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n",
      "\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n",
      "\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n",
      "\u001b[0;32m    819\u001b[0m         )\n",
      "\u001b[0;32m    820\u001b[0m     )\n",
      "\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n",
      "\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n",
      "\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n",
      "\u001b[0;32m    825\u001b[0m         X,\n",
      "\u001b[0;32m    826\u001b[0m         y,\n",
      "\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n",
      "\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n",
      "\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n",
      "\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n",
      "\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n",
      "\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n",
      "\u001b[0;32m    833\u001b[0m     )\n",
      "\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n",
      "\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n",
      "\u001b[0;32m    836\u001b[0m     )\n",
      "\u001b[0;32m    837\u001b[0m )\n",
      "\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m    844\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n",
      "\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n",
      "\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n",
      "\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n",
      "\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n",
      "\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n",
      "\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n",
      "\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n",
      "\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n",
      "\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n",
      "\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n",
      "\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n",
      "\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n",
      "\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\SGM\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n",
      "\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n",
      "\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n",
      "\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n",
      "\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n",
      "\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n",
      "\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n",
      "\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "# Paso 4: Crear el modelo de RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Paso 5: Configurar GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=3\n",
    ")\n",
    "\n",
    "# Paso 6: Ajustar el modelo utilizando GridSearchCV\n",
    "grid_search.fit(x_train_pca_df, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
